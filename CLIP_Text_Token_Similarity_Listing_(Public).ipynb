{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CLIP Text Token Similarity Listing (Public)",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jstenner/ainotebooks/blob/main/CLIP_Text_Token_Similarity_Listing_(Public).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This group installs and loads things\n",
        "Open this up if you want to change CLIP's model or some other guts, otherwise just run it."
      ],
      "metadata": {
        "id": "g2_M3vksZrm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install --no-deps git+https://github.com/openai/CLIP.git\n",
        "%pip install --no-deps ftfy regex tqdm"
      ],
      "metadata": {
        "id": "igvoYYhmRJhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q36ocM1E18Ir"
      },
      "source": [
        "import torch\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "def clear_mem():\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "perceptor, clip_preprocess = clip.load('ViT-B/16')\n",
        "#perceptor, clip_preprocess = clip.load('ViT-B/32')\n",
        "perceptor.eval().float().requires_grad_(False);\n",
        "\n",
        "tokenizer = clip.simple_tokenizer.SimpleTokenizer()"
      ],
      "metadata": {
        "id": "t0044Oto0Qzm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b7a46f2-cd8f-4df5-9d99-5c3fc2074251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 335M/335M [00:03<00:00, 101MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similar_words(target_word=\"cool\", topk=32):\n",
        "    target_tokens = tokenizer.encode(target_word)\n",
        "    if len(target_tokens) > 1:\n",
        "        print(\"This word uses more than one token, can't use it!\")\n",
        "        return\n",
        "    target_emb = perceptor.token_embedding.weight[target_tokens[0],None].detach()\n",
        "    token_sim  = torch.cosine_similarity(target_emb,perceptor.token_embedding.weight.detach(),-1)\n",
        "    top_token_sim = torch.topk(token_sim,topk+1,-1,True,True)\n",
        "    top_indices = top_token_sim.indices[1:]\n",
        "    top_values  = top_token_sim.values[1:]\n",
        "    for i in range(top_indices.shape[0]):\n",
        "        print('\"'+tokenizer.decode([top_indices[i].item()])+'\"',\"   \", top_values[i].item())\n",
        "    return"
      ],
      "metadata": {
        "id": "KMRbY2hyVYII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def similar_words_fancy(target_word=\"cool\", topk=32):\n",
        "    target_tokens = tokenizer.encode(target_word)\n",
        "    if len(target_tokens) > 1:\n",
        "        print(\"This word uses more than one token, can't use it!\")\n",
        "        return\n",
        "    target_emb = perceptor.token_embedding.weight[target_tokens[0],None].detach()\n",
        "    token_sim  = torch.cosine_similarity(target_emb,perceptor.token_embedding.weight.detach(),-1)\n",
        "    top_token_sim = torch.topk(token_sim,topk+1,-1,True,True)\n",
        "    top_indices = top_token_sim.indices[1:]\n",
        "    top_values  = top_token_sim.values[1:]\n",
        "    output = []\n",
        "    for i in range(top_indices.shape[0]):\n",
        "        output.append([tokenizer.decode([top_indices[i].item()]), top_values[i].item()]) \n",
        "\n",
        "    table_build = \"\"\n",
        "    for i in range(len(output)):\n",
        "        table_build = table_build + \"<tr><td>\"+output[i][0]+\"</td><td>\"+str(output[i][1])+\"</td></tr>\"\n",
        "\n",
        "    table_built = \"\"\"\n",
        "    <style>\n",
        "    #output-body {\n",
        "        display: flex;\n",
        "        align-items: left;\n",
        "        justify-content: left;\n",
        "    }\n",
        "    th {\n",
        "      text-align: left;\n",
        "    }\n",
        "    .treecolumn{\n",
        "        column-count: 4;\n",
        "    }\n",
        "    </style>\n",
        "    <div class=\"treecolumn\">\n",
        "    <table style=\"width:100%\">\n",
        "    \"\"\" + table_build + \"\"\"\n",
        "    </table>\n",
        "    </div>\n",
        "    \"\"\"\n",
        "    display(HTML(table_built))"
      ],
      "metadata": {
        "id": "RlUjOm_rbgfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Play Area"
      ],
      "metadata": {
        "id": "xRNAR9XmZ_9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Best matching tokens\n",
        "#@markdown Limited to the topk tokens, listed as decoded text and its cosine similarity to the input.\n",
        "#@markdown <br>Some tokens might be repeated with or without a space after the text (\"cool\" is different from \"cool \" as one is a part of a word and the other is a single word)\n",
        "#@markdown <br>List reads top-down then left-right. CSS + Table issues.\n",
        "\n",
        "word = \"uwu\" #@param {type:\"string\"}\n",
        "topk = 16 #@param {type:\"integer\"}\n",
        "\n",
        "#similar_words(word, topk)\n",
        "similar_words_fancy(word, topk)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "PmbymJFPaP4K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "2f86e5e4-6c03-4917-8ecf-823304537d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    #output-body {\n",
              "        display: flex;\n",
              "        align-items: left;\n",
              "        justify-content: left;\n",
              "    }\n",
              "    th {\n",
              "      text-align: left;\n",
              "    }\n",
              "    .treecolumn{\n",
              "        column-count: 4;\n",
              "    }\n",
              "    </style>\n",
              "    <div class=\"treecolumn\">\n",
              "    <table style=\"width:100%\">\n",
              "    <tr><td>owo </td><td>0.3114655911922455</td></tr><tr><td>ü•∫ </td><td>0.2836507558822632</td></tr><tr><td>:> </td><td>0.2684445083141327</td></tr><tr><td>üò≠üíï </td><td>0.24455787241458893</td></tr><tr><td>ü•≥ </td><td>0.2379431128501892</td></tr><tr><td>üò≠üò≠ </td><td>0.23611247539520264</td></tr><tr><td>„Öã„Öã„Öã </td><td>0.232012540102005</td></tr><tr><td>üò≠üò≠üò≠ </td><td>0.23071454465389252</td></tr><tr><td>smol </td><td>0.23033389449119568</td></tr><tr><td>ü•∫</td><td>0.2263646274805069</td></tr><tr><td>„Ö†„Ö† </td><td>0.2253875732421875</td></tr><tr><td>^^ </td><td>0.22269271314144135</td></tr><tr><td>oof </td><td>0.2204439789056778</td></tr><tr><td>üíï </td><td>0.21993005275726318</td></tr><tr><td>hehe </td><td>0.21951451897621155</td></tr><tr><td>‚ò∫Ô∏è </td><td>0.21945834159851074</td></tr>\n",
              "    </table>\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}